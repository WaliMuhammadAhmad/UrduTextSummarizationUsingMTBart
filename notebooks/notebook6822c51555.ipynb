{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-20T18:31:48.297727Z",
     "iopub.status.busy": "2024-04-20T18:31:48.297273Z",
     "iopub.status.idle": "2024-04-20T18:32:23.547434Z",
     "shell.execute_reply": "2024-04-20T18:32:23.545720Z",
     "shell.execute_reply.started": "2024-04-20T18:31:48.297691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets evaluate tqdm -q\n",
    "!pip install -U accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T17:35:56.736860Z",
     "iopub.status.busy": "2024-04-20T17:35:56.736378Z",
     "iopub.status.idle": "2024-04-20T17:36:16.441140Z",
     "shell.execute_reply": "2024-04-20T17:36:16.439553Z",
     "shell.execute_reply.started": "2024-04-20T17:35:56.736820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T17:36:16.443356Z",
     "iopub.status.busy": "2024-04-20T17:36:16.442957Z",
     "iopub.status.idle": "2024-04-20T17:36:32.667600Z",
     "shell.execute_reply": "2024-04-20T17:36:32.666146Z",
     "shell.execute_reply.started": "2024-04-20T17:36:16.443321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T17:36:33.949854Z",
     "iopub.status.busy": "2024-04-20T17:36:33.949360Z",
     "iopub.status.idle": "2024-04-20T17:37:11.919371Z",
     "shell.execute_reply": "2024-04-20T17:37:11.917941Z",
     "shell.execute_reply.started": "2024-04-20T17:36:33.949820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# Replace 'your_file_id' with the actual file ID from Google Drive\n",
    "file_id = '16KcQSmdQ7dtQHgcQkP8I4IvSsd9lNqYK'\n",
    "output_path = '/kaggle/working/mbart.zip'\n",
    "\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T17:37:16.731774Z",
     "iopub.status.busy": "2024-04-20T17:37:16.731190Z",
     "iopub.status.idle": "2024-04-20T17:37:45.809588Z",
     "shell.execute_reply": "2024-04-20T17:37:45.808162Z",
     "shell.execute_reply.started": "2024-04-20T17:37:16.731740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Unzip the model\n",
    "!unzip /kaggle/working/mbart.zip -d /kaggle/working/mbart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:32:23.550817Z",
     "iopub.status.busy": "2024-04-20T18:32:23.550295Z",
     "iopub.status.idle": "2024-04-20T18:32:40.924120Z",
     "shell.execute_reply": "2024-04-20T18:32:40.922782Z",
     "shell.execute_reply.started": "2024-04-20T18:32:23.550763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import ( AutoModelForSeq2SeqLM, AutoTokenizer,MBartForConditionalGeneration,\n",
    "                         pipeline)\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:32:40.927631Z",
     "iopub.status.busy": "2024-04-20T18:32:40.926811Z",
     "iopub.status.idle": "2024-04-20T18:32:40.936261Z",
     "shell.execute_reply": "2024-04-20T18:32:40.934385Z",
     "shell.execute_reply.started": "2024-04-20T18:32:40.927587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_summary(generated_text):\n",
    "    prefix_to_remove = \"Summary :\"\n",
    "    prefix_to_remove2 = \"Summary:\"\n",
    "    generated_text=str(generated_text)\n",
    "    if generated_text.startswith(prefix_to_remove):\n",
    "        cleaned_text = generated_text[len(prefix_to_remove):]\n",
    "    else:\n",
    "        cleaned_text = generated_text\n",
    "    if generated_text.startswith(prefix_to_remove2):\n",
    "        cleaned_text2 = cleaned_text[len(prefix_to_remove2):]\n",
    "    else:\n",
    "        cleaned_text2 = cleaned_text\n",
    "    return cleaned_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:32:40.938344Z",
     "iopub.status.busy": "2024-04-20T18:32:40.937904Z",
     "iopub.status.idle": "2024-04-20T18:32:40.956690Z",
     "shell.execute_reply": "2024-04-20T18:32:40.955015Z",
     "shell.execute_reply.started": "2024-04-20T18:32:40.938307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_news_prompt(prompt, max_tensor_size=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"tf\")\n",
    "\n",
    "    # Get the size of the input tensor\n",
    "    tensor_size = inputs[\"input_ids\"].shape[1]\n",
    "    print(tensor_size)\n",
    "    # Check if the tensor size is greater than 1024\n",
    "    if tensor_size > max_tensor_size:\n",
    "        # Shorten the prompt to fit within 1024 tokens\n",
    "        num_tokens_to_remove = tensor_size - max_tensor_size\n",
    "        prompt = tokenizer.decode(inputs[\"input_ids\"][0, :-num_tokens_to_remove])\n",
    "    prefix_to_remove = \"en_XX \"\n",
    "    if prompt.startswith(prefix_to_remove):\n",
    "        cleaned_text = prompt[len(prefix_to_remove):]\n",
    "    else:\n",
    "        cleaned_text = prompt\n",
    "    # Return the reduced prompt as text\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:32:40.959536Z",
     "iopub.status.busy": "2024-04-20T18:32:40.959095Z",
     "iopub.status.idle": "2024-04-20T18:32:40.972576Z",
     "shell.execute_reply": "2024-04-20T18:32:40.971092Z",
     "shell.execute_reply.started": "2024-04-20T18:32:40.959501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,MBartForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:32:40.974676Z",
     "iopub.status.busy": "2024-04-20T18:32:40.974273Z",
     "iopub.status.idle": "2024-04-20T18:32:50.282411Z",
     "shell.execute_reply": "2024-04-20T18:32:50.280895Z",
     "shell.execute_reply.started": "2024-04-20T18:32:40.974644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"/kaggle/working/mbart/finetunedmBartLarge67kWith50news147Scripts\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T17:49:17.428410Z",
     "iopub.status.busy": "2024-04-20T17:49:17.427980Z",
     "iopub.status.idle": "2024-04-20T17:49:17.442853Z",
     "shell.execute_reply": "2024-04-20T17:49:17.441618Z",
     "shell.execute_reply.started": "2024-04-20T17:49:17.428381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Text :اردو زبان کو اُردو تختی اور جموں کشمیر کی ادبی زبان قرار دیا جاتا ہے۔ یہ زبان مشرقی پاکستان میں بولی جاتی ہے اور اس کے علاوہ ہندوستان کے کئی حصوں میں بھی استعمال ہوتی ہے۔ اردو کا تعلق عربی اور فارسی زبانوں سے ہے، اور اس کا لکھائی ادبی عربی سے مشابہت رکھتا ہے۔ اس کی لکھائی نستعلیق، نستعلیق نگار، اور نستعلیقِ نگار نستعلیق کہلاتی ہیں۔\n",
    "\n",
    "اردو زبان کی تاریخ بہت پرانی ہے اور اس کی شروعات قدیم ہندوستانی زبانوں سے ملتی ہے۔ اردو کی اختصاری شکل ہے 'اردو' جو 'اردو زبان کا مخفف ہے۔ اردو میں نثر اور نظم کا بہترین اظہار کیا جاتا ہے، اور اس زبان کے شاعر وادیئے، میرزا غالب، اور فیض احمد فیض جیسے معروف ادیب بڑے مقام حاصل کر چکے ہیں۔\n",
    "\n",
    "اس دورانِ حاضر، ٹیکنالوجی کے فیصلے کرنے والے لوگ بھی اردو کے استعمال کے فوائد سمجھ رہے ہیں۔ اردو میں ٹیکسٹ سمارٹ فون، کمپیوٹر، اور انٹرنیٹ کی مختلف اپلیکیشنز کے لئے استعمال ہوتا ہے۔ اردو کے لئے بھی مخصوص نئے الفاظ اور ٹیکنالوجی کے مصطلحات تیار کیے جا رہے ہیں تاکہ زبان کی ترویج اور ترقی میں مدد فراہم کی جائے۔\"\"\"\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:48:57.810582Z",
     "iopub.status.busy": "2024-04-20T18:48:57.810041Z",
     "iopub.status.idle": "2024-04-20T18:48:57.819178Z",
     "shell.execute_reply": "2024-04-20T18:48:57.817568Z",
     "shell.execute_reply.started": "2024-04-20T18:48:57.810545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"وسائیڈل :م: کی لسٹ میں آتے ہوں-ٹھیک ہے مطلب سوسائیڈ سے ریلیٹڈ ہوں جیسے خود کشی میں نے بتایا یا پھر کوئی ہارم بتا دیا اپنے آپ کو نقصان پہنچانا-جی-سیلف-سیلف انجری ہو گیا-جی-سیلف ڈسٹرکشن ہو گئی-اس طرح-جی-اس طرح کے کی ورڈز جی جی بالکل ایسا ہی ہے-(())-ٹھیک ہے سر-یہ بھی دھیان رکھنا ہے آپ لوگوں نے کہ اس کی لسٹ جو ہے ناں وہ لمیٹڈ نہ ہو کافی ویریئیشن آئے ڈائیورسٹی ہے تاکہ ہمارے پاس ایک :م: کافی ذیادہ ناں :م: وہ لسٹ بن جائے ورڈز کی-اور ہمارے لیے-((ایزی))-ہو جائے-((گا))-جی سر :م: یعنی اس میں بائسنیس نہ آئے پھر-جی جی بالکل بائسنیس نہ-<background> </background> بائسنیس-تو ہمارے لیے کافی زیادہ کام خراب ہو گا-:م: مطلب یہ ہے کہ آپ لوگ جو ڈیٹا سیٹ بنا رہے ہیں ناں وہ بالکل ایکوریٹ اور بالکل :م: سمجھ لیں اچھا بنانا ٹھیک بنانا-:م: اوکے سر ہم پوری کوشش کریں گے اپنی-ہاں جی اس خیال کا :م: یہ بھی دھیان رکھنا ہے کہ ماڈل جو ہے وہ ایکوریٹلی لرن کرے-ٹھیک ہے سر-<cough/>-:م: ٹھیک-((ہے))-اوکے :م: ہاں ہاں جب آپ کو پروفیشنلز کی :م: ضرورت پڑے آپ لوگوں نے مجھے بتانا ہے تاکہ ہم ناں ان سے تھوڑی سی گائیڈنس لے سکے اگر ہمارے سے کوئی مسٹیک ہو جائے-ج\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:48:57.968231Z",
     "iopub.status.busy": "2024-04-20T18:48:57.967750Z",
     "iopub.status.idle": "2024-04-20T18:48:57.977078Z",
     "shell.execute_reply": "2024-04-20T18:48:57.975743Z",
     "shell.execute_reply.started": "2024-04-20T18:48:57.968194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prompt=dataset_test['Question'][9]\n",
    "prompt='Text :'+prompt\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:48:58.115589Z",
     "iopub.status.busy": "2024-04-20T18:48:58.115100Z",
     "iopub.status.idle": "2024-04-20T18:48:58.135236Z",
     "shell.execute_reply": "2024-04-20T18:48:58.133750Z",
     "shell.execute_reply.started": "2024-04-20T18:48:58.115554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "size=tokenizer(prompt, return_tensors=\"tf\")[\"input_ids\"].shape[1]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:48:58.301547Z",
     "iopub.status.busy": "2024-04-20T18:48:58.301048Z",
     "iopub.status.idle": "2024-04-20T18:48:58.307752Z",
     "shell.execute_reply": "2024-04-20T18:48:58.306064Z",
     "shell.execute_reply.started": "2024-04-20T18:48:58.301511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:48:58.475494Z",
     "iopub.status.busy": "2024-04-20T18:48:58.475014Z",
     "iopub.status.idle": "2024-04-20T18:48:58.913527Z",
     "shell.execute_reply": "2024-04-20T18:48:58.912226Z",
     "shell.execute_reply.started": "2024-04-20T18:48:58.475446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:48:59.011780Z",
     "iopub.status.busy": "2024-04-20T18:48:59.011373Z",
     "iopub.status.idle": "2024-04-20T18:49:15.678730Z",
     "shell.execute_reply": "2024-04-20T18:49:15.677503Z",
     "shell.execute_reply.started": "2024-04-20T18:48:59.011749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = pipeline('text2text-generation', model=model, tokenizer=tokenizer, do_sample=False)\n",
    "result = generator(prompt, max_length=size+100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T18:49:15.681428Z",
     "iopub.status.busy": "2024-04-20T18:49:15.680973Z",
     "iopub.status.idle": "2024-04-20T18:49:15.689678Z",
     "shell.execute_reply": "2024-04-20T18:49:15.688031Z",
     "shell.execute_reply.started": "2024-04-20T18:49:15.681394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T19:24:10.191344Z",
     "iopub.status.busy": "2024-04-20T19:24:10.190839Z",
     "iopub.status.idle": "2024-04-20T20:33:31.269703Z",
     "shell.execute_reply": "2024-04-20T20:33:31.266195Z",
     "shell.execute_reply.started": "2024-04-20T19:24:10.191304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load your custom model and tokenizer\n",
    "model_path = \"/kaggle/working/mbart/finetunedmBartLarge67kWith50news147Scripts\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Create a directory to store the modified CSV files\n",
    "output_directory = \"/kaggle/working/output\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(input_path, output_directory):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # Generate output based on the first column's content for each row\n",
    "    for index, row in df.iterrows():\n",
    "        input_text = row['reference']\n",
    "        output_text = generator(input_text)[0]['generated_text']\n",
    "        \n",
    "        # Update the 'Output' column with the generated text\n",
    "        df.at[index, 'Output'] = output_text\n",
    "        \n",
    "        # Print a message for each row processed\n",
    "        print(f\"Processed row {index + 1} in file: {input_path}\")\n",
    "    \n",
    "    # Create the output path for the modified CSV file\n",
    "    output_path = os.path.join(output_directory, os.path.basename(input_path))\n",
    "    \n",
    "    # Save the modified CSV file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Print a message when processing is complete for the file\n",
    "    print(f\"Processing complete for file: {input_path}\")\n",
    "\n",
    "# Directory containing the CSV files\n",
    "input_directory = \"/kaggle/input/my-test/Test_files\"\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_directory, filename)\n",
    "        process_csv(input_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:17:20.956912Z",
     "iopub.status.busy": "2024-04-21T04:17:20.956415Z",
     "iopub.status.idle": "2024-04-21T04:17:21.020115Z",
     "shell.execute_reply": "2024-04-21T04:17:21.018887Z",
     "shell.execute_reply.started": "2024-04-21T04:17:20.956879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to remove prefixes from the second column\n",
    "def remove_prefix(text, prefix_list):\n",
    "    for prefix in prefix_list:\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):].strip()\n",
    "    return text\n",
    "\n",
    "# Function to process each CSV file and remove prefixes from the second column\n",
    "def process_csv(input_path, output_directory, prefix_list):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # Remove prefixes from the second column\n",
    "    df['Output'] = df['Output'].apply(lambda x: remove_prefix(x, prefix_list))\n",
    "    \n",
    "    # Create the output path for the modified CSV file\n",
    "    output_path = os.path.join(output_directory, os.path.basename(input_path))\n",
    "    \n",
    "    # Save the modified CSV file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Print a message when processing is complete for the file\n",
    "    print(f\"Processing complete for file: {input_path}\")\n",
    "\n",
    "# Directory containing the CSV files\n",
    "input_directory = \"/kaggle/working/output\"\n",
    "output_directory = \"/kaggle/working/removed_prefix_output2\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# List of prefixes to remove\n",
    "prefix_list = [\"Summary :\", \"Summary \", \"Summary Summary : \", \"Summary:\", \"Summary Summary:\"]\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_directory, filename)\n",
    "        process_csv(input_path, output_directory, prefix_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:17:25.820838Z",
     "iopub.status.busy": "2024-04-21T04:17:25.820027Z",
     "iopub.status.idle": "2024-04-21T04:17:25.832446Z",
     "shell.execute_reply": "2024-04-21T04:17:25.830973Z",
     "shell.execute_reply.started": "2024-04-21T04:17:25.820792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "csv_file_path = \"/kaggle/working/removed_prefix_output/MR240-test.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:11:00.133696Z",
     "iopub.status.busy": "2024-04-21T04:11:00.133292Z",
     "iopub.status.idle": "2024-04-21T04:11:00.172575Z",
     "shell.execute_reply": "2024-04-21T04:11:00.171458Z",
     "shell.execute_reply.started": "2024-04-21T04:11:00.133662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def zip_directory(directory, zip_name):\n",
    "    # Create a zip file containing all files and subdirectories within the directory\n",
    "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))\n",
    "\n",
    "# Directory to be zipped and downloaded\n",
    "directory_to_download = \"/kaggle/working/removed_prefix_output\"\n",
    "\n",
    "# Name for the zip file\n",
    "zip_file_name = \"downloaded_directory.zip\"\n",
    "\n",
    "# Zip the directory\n",
    "zip_directory(directory_to_download, zip_file_name)\n",
    "\n",
    "# Provide the zip file for download\n",
    "FileLink(zip_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:14:43.377240Z",
     "iopub.status.busy": "2024-04-21T04:14:43.376868Z",
     "iopub.status.idle": "2024-04-21T04:14:43.390057Z",
     "shell.execute_reply": "2024-04-21T04:14:43.388616Z",
     "shell.execute_reply.started": "2024-04-21T04:14:43.377213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the CSV files\n",
    "input_directory = \"/kaggle/working/removed_prefix_output\"\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Read the CSV file and extract the data from the second column\n",
    "        with open(input_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            second_column = [line.split(',')[1].strip() for line in lines[1:]]  # Assuming CSV format\n",
    "            \n",
    "            # Concatenate the data from the second column into a single string\n",
    "            concatenated_data = '\\n'.join(second_column)\n",
    "            \n",
    "            # Write the concatenated data to a text file\n",
    "            output_file_path = os.path.splitext(input_path)[0] + \"_concatenated.txt\"\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.write(concatenated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:16:10.750791Z",
     "iopub.status.busy": "2024-04-21T04:16:10.750108Z",
     "iopub.status.idle": "2024-04-21T04:16:10.757245Z",
     "shell.execute_reply": "2024-04-21T04:16:10.756116Z",
     "shell.execute_reply.started": "2024-04-21T04:16:10.750759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "txt_file_path = \"/kaggle/working/removed_prefix_output/MR241-test_concatenated.txt\"\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(txt_file_path, 'r') as file:\n",
    "    file_contents = file.read()\n",
    "\n",
    "# Display the contents of the text file\n",
    "print(file_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:19:44.638742Z",
     "iopub.status.busy": "2024-04-21T04:19:44.638073Z",
     "iopub.status.idle": "2024-04-21T04:19:44.650768Z",
     "shell.execute_reply": "2024-04-21T04:19:44.649521Z",
     "shell.execute_reply.started": "2024-04-21T04:19:44.638699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the CSV files\n",
    "input_directory = \"/kaggle/working/removed_prefix_output2\"\n",
    "\n",
    "# Directory to store the output text files\n",
    "output_directory = \"/kaggle/working/concatenated_files2\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Read the CSV file and extract the data from the second column\n",
    "        with open(input_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            second_column = [line.split(',')[1].strip() for line in lines[1:]]  # Assuming CSV format\n",
    "            \n",
    "            # Concatenate the data from the second column into a single string\n",
    "            concatenated_data = '\\n'.join(second_column)\n",
    "            \n",
    "            # Specify the output file path in the output directory\n",
    "            output_file_path = os.path.join(output_directory, os.path.splitext(filename)[0] + \"_concatenated.txt\")\n",
    "            \n",
    "            # Write the concatenated data to a text file in the output directory\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.write(concatenated_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now remove the lines among the lines of each file and output in a separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:58:54.576779Z",
     "iopub.status.busy": "2024-04-21T04:58:54.576337Z",
     "iopub.status.idle": "2024-04-21T04:58:54.588474Z",
     "shell.execute_reply": "2024-04-21T04:58:54.587583Z",
     "shell.execute_reply.started": "2024-04-21T04:58:54.576748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to remove spaces and newlines between lines\n",
    "def join_lines(lines):\n",
    "    return ' '.join(line.strip() for line in lines)\n",
    "\n",
    "# Directory containing the text files\n",
    "input_directory = \"/kaggle/working/concatenated_files2\"\n",
    "\n",
    "# Directory to store the output text files\n",
    "output_directory = \"/kaggle/working/concatenated_paragraphs2\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each text file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Read the text file and join lines into a single paragraph\n",
    "        with open(input_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            paragraph = join_lines(lines)\n",
    "            \n",
    "            # Specify the output file path in the output directory\n",
    "            output_file_path = os.path.join(output_directory, os.path.splitext(filename)[0] + \"_paragraph.txt\")\n",
    "            \n",
    "            # Write the paragraph to a text file in the output directory\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.write(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T04:59:32.386216Z",
     "iopub.status.busy": "2024-04-21T04:59:32.385819Z",
     "iopub.status.idle": "2024-04-21T04:59:32.393034Z",
     "shell.execute_reply": "2024-04-21T04:59:32.391879Z",
     "shell.execute_reply.started": "2024-04-21T04:59:32.386190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Specify the path to the text file\n",
    "txt_file_path = \"/kaggle/working/concatenated_paragraphs2/MR053_test_concatenated_paragraph.txt\"\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(txt_file_path, 'r') as file:\n",
    "    file_contents = file.read()\n",
    "\n",
    "# Display the contents of the text file\n",
    "print(file_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:00:15.864068Z",
     "iopub.status.busy": "2024-04-21T05:00:15.863664Z",
     "iopub.status.idle": "2024-04-21T05:00:15.879161Z",
     "shell.execute_reply": "2024-04-21T05:00:15.878121Z",
     "shell.execute_reply.started": "2024-04-21T05:00:15.864038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def zip_directory(directory, zip_name):\n",
    "    # Create a zip file containing all files and subdirectories within the directory\n",
    "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))\n",
    "\n",
    "# Directory to be zipped and downloaded\n",
    "directory_to_download = \"/kaggle/working/concatenated_paragraphs2\"\n",
    "\n",
    "# Name for the zip file\n",
    "zip_file_name = \"mBart-summaries.zip\"\n",
    "\n",
    "# Zip the directory\n",
    "zip_directory(directory_to_download, zip_file_name)\n",
    "\n",
    "# Provide the zip file for download\n",
    "FileLink(zip_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:20:57.235804Z",
     "iopub.status.busy": "2024-04-21T05:20:57.234839Z",
     "iopub.status.idle": "2024-04-21T05:21:13.610362Z",
     "shell.execute_reply": "2024-04-21T05:21:13.608910Z",
     "shell.execute_reply.started": "2024-04-21T05:20:57.235769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:22:01.643125Z",
     "iopub.status.busy": "2024-04-21T05:22:01.642703Z",
     "iopub.status.idle": "2024-04-21T05:22:15.089973Z",
     "shell.execute_reply": "2024-04-21T05:22:15.088766Z",
     "shell.execute_reply.started": "2024-04-21T05:22:01.643098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load your custom model and tokenizer\n",
    "model_path = \"/kaggle/working/mbart/finetunedmBartLarge67kWith50news147Scripts\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text2text-generation', model=model, tokenizer=tokenizer, do_sample=False)\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(reference, generated):\n",
    "    reference_tokens = tokenize_text(reference)\n",
    "    generated_tokens = tokenize_text(generated)\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(reference, generated):\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores['rouge1'].fmeasure, scores['rouge2'].fmeasure, scores['rougeL'].fmeasure\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"/kaggle/input/mbart-comparison-csv/mBart-comparison.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Apply the functions to each row and create new columns\n",
    "df['BLEU'] = df.apply(lambda row: calculate_bleu(row['reference'], row['generated']), axis=1)\n",
    "df['Rouge-1'], df['Rouge-2'], df['Rouge-L'] = zip(*df.apply(lambda row: calculate_rouge(row['reference'], row['generated']), axis=1))\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "output_csv_file_path = \"/kaggle/working/all.csv\"\n",
    "df.to_csv(output_csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:27:32.799987Z",
     "iopub.status.busy": "2024-04-21T05:27:32.798157Z",
     "iopub.status.idle": "2024-04-21T05:27:42.781356Z",
     "shell.execute_reply": "2024-04-21T05:27:42.780185Z",
     "shell.execute_reply.started": "2024-04-21T05:27:32.799923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load your custom model and tokenizer\n",
    "model_path = \"/kaggle/working/mbart/finetunedmBartLarge67kWith50news147Scripts\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(texts):\n",
    "    return [tokenizer(sent, return_tensors=\"pt\")[\"input_ids\"] for sent in texts]\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(reference, generated):\n",
    "    reference_tokens = tokenize_text(reference)\n",
    "    generated_tokens = tokenize_text(generated)\n",
    "    return sentence_bleu(reference_tokens, generated_tokens)\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(reference, generated):\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores['rouge1'].fmeasure, scores['rouge2'].fmeasure, scores['rougeL'].fmeasure\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"/kaggle/input/mbart-comparison-csv/mBart-comparison.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Apply the functions to each row and create new columns\n",
    "df['BLEU'] = df.apply(lambda row: calculate_bleu([row['reference']], [row['generated']]), axis=1)\n",
    "df['Rouge-1'], df['Rouge-2'], df['Rouge-L'] = zip(*df.apply(lambda row: calculate_rouge(row['reference'], row['generated']), axis=1))\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "output_csv_file_path = \"/kaggle/working/all2.csv\"\n",
    "df.to_csv(output_csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4840812,
     "sourceId": 8177631,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4840864,
     "sourceId": 8177704,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4843644,
     "sourceId": 8181200,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
